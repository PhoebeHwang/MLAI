# -*- coding: utf-8 -*-
"""Tokenizer and TfidfVectorizer.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gc5XXOTUHeaRTpB5mn4f0a_AF49r4kpu

문제1) Tokenizer 생성하기
"""

from re import T
class Tokenizer():
  def __init__(self):
    self.word_dict = {'oov': 0}
    self.fit_checker = False
  
  def preprocessing(self, sequences):
    result = []
    
    import re
    for i in sequences:
        output1 = i.lower()
        output2 = re.sub("[^a-zA-Z0-9 ]","",output1)

        result.append(output2.split())  

    return result
  
  def fit(self, sequences):
    self.fit_checker = False

    tokens = self.preprocessing(sequences)
    word_set = sum(tokens,[])
    self.word_dict = {w:i for i, w in enumerate(word_set)}
    self.word_dict['oov'] = 0

    return self.word_dict

    self.fit_checker = True
  
  def transform(self, sequences):
    result = []
    tokens = self.preprocessing(sequences)
    #if self.fit_checker:
    for w in tokens :
        temp = []
        for i in w :
            try :
                temp.append(self.word_dict[i])
            except KeyError :
                temp.append(self.word_dict['oov'])
        result.append(temp)

    return result
    #else:
      #raise Exception("Tokenizer instance is not fitted yet.")


  def fit_transform(self, sequences):
    self.fit(sequences)
    result = self.transform(sequences)
    return result

a = Tokenizer()
text = ['I go to school.', 'I LIKE pizza!']
text2 = ['I hate you.', 'me too.']

a.preprocessing(text)

a.fit(text)

a.transform(text)

"""문제2) TfidfVectorizer 생성하기"""

class TfidfVectorizer():

  def __init__(self,tokenizer):
    self.tokenizer = tokenizer
    self.fit_checker = False
  
  def fit(self, sequences):
    tokenized = self.tokenizer.fit_transform(sequences)
    tokenized_word = self.tokenizer.preprocessing(sequences)
    word_set = sum(tokenized_word,[])

    import pandas as pd
    from math import log
    
    #전체 문장수
    n = len(sequences)
    #특정문장d에서 특정단어 t의 등장횟수
    def tf(d,t):
        return d.count(t)
    #df(d,t) : 특정 단어t가 등장한 문장d의 개수
    #df(t)에 반비례하는 수
    def idf(t):
        df = 0
        for d in tokenized_word:
            df += t in d
        return log(n/(df+1))

    def tfidf(d,t):
        return tf(d,t)*idf(t)

    result = []

    for j in range(len(word_set)):
        t = word_set[j]
        result.append(idf(t))

    return result

    self.fit_checker = True

  def transform(self, sequences):
    #if self.fit_checker:
    tokenized = self.tokenizer.transform(sequences)
    tokenized_word = self.tokenizer.preprocessing(sequences)
    self.fit(sequences)

    word_set = sum(tokenized_word,[])
    word_set_list = set(word_set)
    word_set_list = list(word_set_list)

    import pandas as pd
    from math import log

    n = len(sequences)

    def idf(t):
        df = 0
        for d in tokenized_word:
            df += t in d
        return log(n/(df+1))

    #TF
    result = []
    result2 = []
    
    for i in range(n):
        result.append([])
        result2.append([])
        d = tokenized_word[i]

        for j in range(len(word_set_list)):
            t = word_set_list[j]
            result[-1].append(d.count(t))
            result2[-1].append(d.count(t)*idf(t))
            
    tf_matrix = pd.DataFrame(result, columns = word_set_list)
    tf_matrix.index = sequences

    tfidf_matrix = pd.DataFrame(result2, columns = word_set_list)
    tfidf_matrix.index = sequences

    return tfidf_matrix

    #TF-IDF

    #else:
        #raise Exception("TfidfVectorizer instance is not fitted yet.")


  def fit_transform(self, sequences):
    self.fit(sequences)
    return self.transform(sequences)

sequences = ['I go to school.', 'I LIKE pizza!']
b = TfidfVectorizer(Tokenizer())

b.fit(sequences)

b.transform(sequences)